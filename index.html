<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VideoITG: Improving Multimodal Video Understanding with Instructed Temporal Grounding</h1>
          <!-- <div class="is-size-3 publication-authors">
            UnderReview
          </div> -->
          <!-- <div style="color: red; font-size: 24px; font-weight: bold;">CVPR 2025</div> 添加的红色字体行 -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=7TWugs4AAAAJ">Shihao Wang</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=1VI_oYUAAAAJ&hl=zh-CN&oi=ao">Zhiding Yu*</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Xiaohui Jiang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=jIUI6F4AAAAJ&hl=zh-CN&oi=ao">Shiyi Lan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=lRTuQyAAAAAJ&hl=zh-CN&oi=sra">Min Shi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://nadinechang.com/">Nadine Chang</a><sup>1</sup>,
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=P9FclNEAAAAJ&hl=zh-CN&oi=ao">Jan Kautz</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=xuYqZlUAAAAJ">Ying Li</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=Oyx-_UIAAAAJ">Jose M. Alvarez</a><sup>1</sup>,
            </span>
          </div>

          <div class="organization">
            <span class="author-block">NVIDIA<sup>1</sup>, The Hong Kong Polytechnic University<sup>2</sup>, Beijing Institute of Technology<sup>3</sup></span>
            <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <div class="Equal Corresponding">
            <span class="author-block">*Corresponding Authors</span>
            <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>
	<div class="Email info">
	  <span style="font-size: 1.2em; color: #5e5c5b;">&#9993;</span>
	  <span class="author-block" style="font-size: smaller;">
	    scutchrisding@gmail.com
	  </span>
	</div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.01533"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/NVlabs/OmniDrive"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered">
              <div class="column has-text-centered">
                  <img src="./static/images/teaser.png"
                       class="interpolation-image"
                       alt="Interpolate start reference image."
                       style="width: 100%; height: auto;"/>
                  <p class="has-text-centered" style="margin-top: 10px; font-size: 0.9em; color: #666;">
                  <b>Figure 1: OmniDrive is a holistic vision-language dataset for autonomous driving, utilizing counterfactual reasoning to generate high-quality QA data from simulated and actual trajectories. We explore two baseline models: Omni-Q, which designs vision-language models (VLMs) from a 3D perception standpoint, and Omni-L, which builds from VLMs to enhance 3D integration. </b> 
                  </p>
              </div>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The advances in vision-language models (VLMs) have led to a growing interest in autonomous driving to leverage their strong reasoning capabilities. However, extending these capabilities from 2D to full 3D understanding is crucial for real-world applications. To address this challenge, we propose OmniDrive, a holistic vision-language dataset that aligns agent models with 3D driving tasks through counterfactual reasoning. This approach enhances decision-making by evaluating potential scenarios and their outcomes, similar to human drivers considering alternative actions. Our counterfactual-based synthetic data annotation process generates large-scale, high-quality datasets, providing denser supervision signals that bridge planning trajectories and language-based reasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely Omni-L and Omni-Q, to assess the importance of vision-language alignment versus 3D perception, revealing critical insights into designing effective LLM-agents. Significant improvements on the DriveLM Q&A benchmark and nuScenes open-loop planning demonstrate the effectiveness of our dataset and methods.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
  
    </div>
  </section>

  
<section class="section">
    <div class="container is-max-desktop">
  
  <!-- Quantitative Results. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <center><h2 class="title is-3">OmniDrive Dataset Construction</h2></center>
          <div class="column is-center has-text-centered">
              <img src="./static/images/data_gen.png"
                   class="interpolation-image"
                   alt="Interpolate start reference image."
                   style="transform: scale(0.9);"/>
              <p class="has-text-centered" style="margin-top: 2px; font-size: 0.9em; color: #666;">
                <b>Figure 2: The proposed counterfactual-based synthetic Q&A data generation pipeline integrates semantic key-frame selection, counterfactual-based checklist and prompt design, and human-in-the-loop quality checks to create high-quality Q&A pairs. </b>
              </p>
          </div>
          <!--/ Interpolating. -->
        </div>
      </div>
  
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
  
  <!-- Quantitative Results. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <center><h2 class="title is-3">OmniDrive-Agent</h2></center>
          <div class="column is-center has-text-centered">
              <img src="./static/images/pipeline.png"
                   class="interpolation-image"
                   alt="Interpolate start reference image."
                   style="transform: scale(0.95);"/>
              <p class="has-text-centered" style="margin-top: 2px; font-size: 0.9em; color: #666;">
                <b>Figure 3: Overall pipeline of Omni-L and Omni-Q. The Omni-L. Omni-L follows the 2D VLM design of LLAVA, introducing 3D positional encoding and using MLP layers for vision-language alignment. Omni-Q is based on 3D BEV perception, aligning its architecture with the Q-Former design.</b>
              </p>
          </div>
          <!--/ Interpolating. -->
        </div>
      </div>
  
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
  
  <!-- Quantitative Results. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <center><h2 class="title is-3">Quantitative Results</h2></center>
  
          <!-- Interpolating. -->
          <h3 class="title is-4" style="margin-top: 30px;">Main Results</h3>
          <div class="content has-text-justified">
          </div>
          <div class="column is-center has-text-centered">
              <img src="./static/images/results1.png"
                   class="interpolation-image"
                   alt="Interpolate start reference image."/>
                   <p class="has-text-centered" style="margin-top: 2px; font-size: 0.9em; color: #666;">
                    <b>Table 1: Comparison on nuScenes Open-loop planning.</b>
                  </p>                   
          </div>
  
  
          <br/>
          <!--/ Interpolating. -->
  
          <!-- Re-rendering. -->
          <h3 class="title is-4">Empirical Study on OmniDrive</h3>
          <div class="content has-text-justified">
          </div>
          <div class="columns is-centered">
            <div class="column is-center has-text-centered">
                <img src="./static/images/results2.png"
                     class="interpolation-image"
                     alt="Interpolate start reference image."/>
                     <p class="has-text-centered" style="margin-top: 2px; font-size: 0.9em; color: #666;">
                      <b>Table 2: Empirical study on OmniDrive dataset and OmniDrive agent design.</b>
                    </p>         
            </div>
          </div>
        </div>
      </div>
  
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <center><h2 class="title is-3">Qualitative Results</h2></center><br>
      <h2 class="caption" style="text-align: center; margin-bottom: 20px; font-size: 1.2em; font-weight: bold; margin-left: 50px;">
        OmniDrive is capable of effectively planning and reasoning within 3D space, making it ideal for complex driving scenarios. It can navigate diverse environments, handle dynamic obstacles, and optimize routes in real-time, ensuring safe and efficient travel.
      </h2>
      <div class="content has-text-centered">
        <img src="./static/images/demo1.gif"
             class="interpolation-image"
             alt="Planning example 1."/>
        <p class="has-text-centered" style="margin-top: 2px; font-size: 0.9em; color: #666;">
          <b>Figure 4: OmniDrive-Agent planning example 1.</b>
        </p>
      </div>
      <br/>
      <div class="content has-text-centered">
        <img src="./static/images/demo2.gif"
             class="interpolation-image"
             alt="Planning example 2."/>
        <p class="has-text-centered" style="margin-top: 2px; font-size: 0.9em; color: #666;">
          <b>Figure 5: OmniDrive-Agent planning example 2.</b>
        </p>
      </div>
      <br/>
      <div class="content has-text-centered">
        <img src="./static/images/demo3.gif"
             class="interpolation-image"
             alt="Conversation example."/>
        <p class="has-text-centered" style="margin-top: 2px; font-size: 0.9em; color: #666;">
          <b>Figure 6: OmniDrive-Agent conversation example.</b>
        </p>
      </div>
      <br/>
      <div class="content has-text-centered">
        <img src="./static/images/demo4.png"
             class="interpolation-image"
             alt="Conversation example."/>
        <p class="has-text-centered" style="margin-top: 2px; font-size: 0.9em; color: #666;">
          <b>Figure 7: OmniDrive-Agent counterfactual reasoning example 1.</b>
        </p>
      </div>
      <br/>
      <div class="content has-text-centered">
        <img src="./static/images/demo5.png"
             class="interpolation-image"
             alt="Conversation example."/>
        <p class="has-text-centered" style="margin-top: 2px; font-size: 0.9em; color: #666;">
          <b>Figure 8: OmniDrive-Agent counterfactual reasoning example 2.</b>
        </p>
      </div>
      </div>
    </div>
  </section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{wang2025omnidrive,
      title={Omnidrive: A holistic vision-language dataset for autonomous driving with counterfactual reasoning},
      author={Wang, Shihao and Yu, Zhiding and Jiang, Xiaohui and Lan, Shiyi and Shi, Min and Chang, Nadine and Kautz, Jan and Li, Ying and Alvarez, Jose M},
      booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
      pages={22442--22452},
      year={2025}
    }
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
        href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
      </p>
    </div>
  </div>
</div>
</div>
</footer>

<script>
      var videoContainer = document.getElementById('video-container');
      var video = document.getElementById('video');

      var videoOffset = videoContainer.offsetTop;

      window.addEventListener('scroll', function() {
        var scrollPosition = window.scrollY || window.pageYOffset;

        if (scrollPosition >= videoOffset) {
          video.play();
        } else {
          video.pause();
        }
      });
    </script>

</body>
</html>
